** evaluating KV stores **
- using CAP theorem
define availability measure
- replication
- R/W costs
- node information maintainted in steady state
- message complexity in steady state
- message complexity in inconsistent state
- evaluate performance against non-actor programming languages?


        ** Napster (1999) **
- first system to recognise that popular content demand can be handled my multiple hosts (peer) instead of central server
- peer to peer file storage
- centrilised file search based on lists provided by each peer
- To retrieve a file, a user queries this central server using the desired file’s well known name and obtains the IP address of a user machine storing the requested file. The file is then down-loaded directly from this user machine. [Ratnasamy]
- single point of failure


        ** Gnutella [Chord, CAN]**
- decentrilized search algorithm
- searches are flooded across peers with a limited scope.
	- upon receiving query, peer returns a list of all content matching the query
- The load on each node grows linearly with the total number of queries, which in turn grows with system size, this approach is clearly not scalable

* Gia? *


        ~gen 2~

        ** Tapestry **


        ** Pastry **
- routing: O(log N)
* applications: global data storage, data sharing, group communication and naming

* design
- Each node in the Pastry network has a unique 128-bit numeric identifier (nodeId)
- When presented with a message and a numeric key, a Pastry node efficiently routes the message to the node with a nodeId that is numerically closest to the key, among all currently live Pastry nodes
- Each Pastry node keeps track of its immediate neighbors in the nodeId space, and notifies applications of new node arrivals, node failures and recoveries
- Pastry takes into account network locality; it seeks to minimize the distance messages travel, according to a scalar proximity metric like the number of IP routing hops
- Despite concurrent node failures, eventual delivery is guaranteed unless |L|/2 nodes with adjacent nodeIds fail simultaneously (|L| is a configuration parameter with a typical value of 16 or 32).
- In each routing step, a node normally forwards the message to a node whose nodeId shares with the key a prefix that is at least one digit (or b bits) longer than the prefix that the key shares with the present node’s id. If no such node is known, the message is forwarded to a node whose nodeId shares a prefix with the key as long as the current node, but is numerically closer to the key than the present node’s id. 

        ** CAN [Ratnasamy]**
- node info: O(d) nodes in a d-dimensional space
- lookup: O(dn^1/d) hops for d-dimensions and n nodes
* applications
- construction of wide-area name resolution services that (unlike the DNS) decouple the naming scheme from the name resolution process thereby enabling arbitrary, location-independent naming schemes
- large scale storage management systems

* design
- CAN is composed of many individual nodes. Each CAN node stores a chunk (called a zone) of the entire hash table
- In addition, a node holds information about a small number of “adjacent” zones in the table
- CRUD Requests for a particular key are routed by intermediate CAN nodes towards the CAN node whose zone contains that key
- a virtual d-dimensional Cartesian coordinate space on a d-torus (a surface of revolution generated by revolving a circle in three-dimensional space about an axis that is coplanar with the circle)
- In a d-dimensional coordinate space, two nodes are neighbors if their coordinate spans overlap along d1 dimensions and abut along one dimension
- To store a pair (K1 ,V1), key K1 is deterministically mapped onto a point P in the coordinate space using a uniform hash function. The corresponding (key,value) pair is then stored at the node that owns the zone within which the point P lies
- To retrieve an entry corresponding to key K1, any node can apply the same deterministic hash function to map K1 onto point P and then retrieve the corresponding value from the point P
- Using its neighbor coordinate set, a node routes a message towards its destination by simple greedy forwarding to the neighbor with coordinates closest to the destination coordinates.
- Node join is done by an existing node splitting its allocated zone in half, retaining half and handing the other half to the new node
- Node departure is done by explicitly handing over the zone and associated KV database to one of its neighbors
- Node failure handled through an immediate takeover algorithm that ensures one of the failed node’s neighbors takes over the zone. However in this case the (key,value) pairs held by the departing node are lost until the state is refreshed by the holders of the data
- Sends heartbeat messages to neighbours. Absence of this over a period of time indicates failure 

        ** Chord **
- a scalable protocol for lookup in a dynamic peer-to-peer system with frequent node arrivals and departures
- Three features that distinguish Chord from many other peer-topeer lookup protocols are its simplicity, provable correctness, and
provable performance
- given a node, maps a node IP to an id
- Given a key, maps the key to a node
- communication cost and state per node scales logarithmically
- node info: (O logN) nodes
- lookup: (O logN) messages
- node changes: (O log2 N) messages

* Key structure
- no naming structure is imposed
- consistent hashing using an m-bit ID (m being possible upper bound on nodes):
	- node ID is created by hashing the IP address
	- key id is created by hashing original key
- assigning keys into node
	- key is assigned to first node whose ID is equal or the next k (successor)
	- successors are identified in a clockwise manner

* environment changes
- node join
	- keys equal to node assigned to node and successor keys for node are given to node
- node departure. (pred, self]
	- all assigned keys are re-assigned to successor

* mitigating adversarial changes
- intentional hashing of keys to the same ID
	- use of SHA-1 function. Making a set of colliding keys is believed hard to accomplish thus it is based on "standard hardness assumptions" that each node is responsible for at most 1 + e)K/N keys



** virtual nodes **
- a solution to balancing machine load allocation in the node ring
- each real machine pretends to be many distinct machines. These operate independently in the DHT ring.

* drawbacks [Karger, Ruhl]
- real node must alocate space for data of each virtual node. More data space. However, this space requirement might be seen as negligable as these p2p structures need logarithmic space per node
- Network bandwidth as every virtual node must frequently do liveness check on neighbours, replacing neighbours if liveness check fails. Many virtual nodes creates an increase in bandwidth load.



        ** DHT based protocols **
* drawbacks
- churn does cause significant overhead for DHTs. [Chawathe, Ratnasamy]
	- to preserve the efficiency and correctness of routing, most DHTs require O(log n) repair operations after each failure.
	- Graceless failures, where a node fails without beforehand informing its neighbors and transferring the relevant state, require more time and work in DHTs to (a) discover the failure and (b) rereplicate the lost data or pointers
	- If the churn rate is too high, the overhead caused by these repair operations can become substantial and could easily overwhelm nodes with low-bandwidth connections


        ** Skip Graph **
Because hashing destroys the ordering on keys, DHT systems do not support queries that seek near matches to a key, or to keys within a given range
Skip graphs can also be constructed without knowledge of the total number of nodes in advance. In contrast, DHT systems such as Pastry and Chord require a priori knowledge about the size of the system or its keyspace



        ** alternative load balancing algorithms for p2p systems [Karger, Ruhl] **
* shortcomings of load balancing
- “random” partition of the address space to nodes is not completely balanced. Some nodes end up responsible for a larger portion of the addresses and thus receive a larger portion of the randomly distributed items.
- some applications may preclude the randomization of data items’ addresses. For example, to support range searching in a database application the items may need
to be placed in a specific order, or even at specific addresses, on the ring. In such cases, we may find the items unevenly distributed in address space, meaning that balancing the address space among nodes does not balance the distribution of items among nodes.
