** evaluating KV stores **
- using CAP theorem
define availability measure
- replication
- R/W costs
- node information maintainted in steady state
- message complexity in steady state
- message complexity in inconsistent state
- evaluate performance against non-actor programming languages?


        ** Napster (1999) **
- first system to recognise that popular content demand can be handled my multiple hosts (peer) instead of central server
- peer to peer file storage
- centrilised file search based on lists provided by each peer
- To retrieve a file, a user queries this central server using the desired file’s well known name and obtains the IP address of a user machine storing the requested file. The file is then down-loaded directly from this user machine. [Ratnasamy]
- single point of failure


        ** Gnutella [Chord, CAN]**
- decentrilized search algorithm
- searches are flooded across peers with a limited scope.
	- upon receiving query, peer returns a list of all content matching the query
- The load on each node grows linearly with the total number of queries, which in turn grows with system size, this approach is clearly not scalable

* Gia? *


        ~gen 2~

        ** Kademlia **
* uses: Ethereum node peer discovery
- Configuration information spreads automatically as a side-effect of key lookups
- Every message a node transmits includes its node ID, permitting the recipient to record the sender’s existence if necessary
- Kademlia uses parallel, asynchronous queries to avoid timeout delays from failed nodes.
- 160 bit random keys for node IDs
- KV pairs are stored on nodes with IDs “close” to the key for some notion of closeness
- a nodeID-based routing algorithm lets anyone efficiently locate servers near any given target key
- unlike Chord, Kademlia, in contrast, can send a query to any node within an interval, allowing it to select routes based on latency or even send parallel, asynchronous queries to several equally appropriate nodes
- To locate nodes near a particular ID, Kademlia uses a single routing algorithm from start to finish.In contrast, other systems use one algorithm to get near the target ID and another for the last few hops
- node failure seems to be tolerated by the fact that data is stored accross k-buckets
- exploits the fact that node failures are inversely related to uptime
- message complexity is "a logarithm" of the network size. if peers join and leave the network rapidly,the lookup structure may never stabilize to a state that enjoys this provably logarithmic bound
- O(logk n) on average

* design
- nodes treated as leaves in a binary tree, with each node’s position determined by the shortest unique prefix of its ID.
- Kademlia protocol ensures that every node knows of at least one node in each of its subtrees, if that subtree contains a node.With this guarantee, any node can locate any other node by its ID
- To assign key,value pairs to particular nodes, Kademlia relies on a notion of distance between two identifiers. Given two 160-bit identifiers, x and y, Kademlia defines the distance between them as their XOR
- Like Chord’s clockwise circle metric, XOR is unidirectional which ensures that all lookups for the same key converge along the same path, regardless of the originating node

* node state
- For each 0 ≤ i < 160, every node keeps a list of {IP address, UDP port, Node ID} triples for nodes of distance between 2i and 2i+1 from itself. (K-Buckets)
- Each k-bucket is kept sorted by time last seen—leastrecently seen node at the head, most recently seen at the tail.
- For large values of i, the lists can grow up to size k, where k is a system-wide replication parameter. k is chosen such that any given k nodes are very unlikely to fail within an hour of each other. Allows for nodes to come and go and still have the value available in some node
- When a Kademlia node receives any message (request or reply) from another node, it updates the appropriate k-bucket for the sender’s node ID
	- If the sending node already exists in the recipient’s k-bucket, the recipient moves it to the tail of the list.
	- If the node is not already in the appropriate k-bucket and the bucket has fewer than k entries, then the recipient just inserts the new sender at the tail of the list
	- If the appropriate k-bucket is full, however, then the recipient pings the k-bucket’s least-recently seen node to decide what to do
	- If the leastrecently seen node fails to respond, it is evicted from the k-bucket and the new sender inserted at the tail.Otherwise, if the least-recently seen node responds, it is moved to the tail of the list, and the new sender’s contact is discarded

* message protocol
- consists of four RPCs:
		- ping:  probes a node to see if it is online
		- store: instructs a node to store a KV pair for later retrieval
		- find_node: returns {IP, UDP port, Node ID} triples for the k nodes it knows about closest to the target ID
		- find_value: behaves like find node except if the RPC recipient has received a store
RPC for the key, it just returns the stored value

        ** Pastry **
- routing: O(log N)
* applications: global data storage, data sharing, group communication and naming

* design
- Each node in the Pastry network has a unique 128-bit numeric identifier (nodeId)
- When presented with a message and a numeric key, a Pastry node efficiently routes the message to the node with a nodeId that is numerically closest to the key, among all currently live Pastry nodes
- Each Pastry node keeps track of its immediate neighbors in the nodeId space, and notifies applications of new node arrivals, node failures and recoveries
- Pastry takes into account network locality; it seeks to minimize the distance messages travel, according to a scalar proximity metric like the number of IP routing hops
- Despite concurrent node failures, eventual delivery is guaranteed unless |L|/2 nodes with adjacent nodeIds fail simultaneously (|L| is a configuration parameter with a typical value of 16 or 32).
- In each routing step, a node normally forwards the message to a node whose nodeId shares with the key a prefix that is at least one digit (or b bits) longer than the prefix that the key shares with the present node’s id. If no such node is known, the message is forwarded to a node whose nodeId shares a prefix with the key as long as the current node, but is numerically closer to the key than the present node’s id. 

        ** CAN [Ratnasamy]**
- node info: O(d) nodes in a d-dimensional space
- lookup: O(dn^1/d) hops for d-dimensions and n nodes
* applications
- construction of wide-area name resolution services that (unlike the DNS) decouple the naming scheme from the name resolution process thereby enabling arbitrary, location-independent naming schemes
- large scale storage management systems

* design
- CAN is composed of many individual nodes. Each CAN node stores a chunk (called a zone) of the entire hash table
- In addition, a node holds information about a small number of “adjacent” zones in the table
- CRUD Requests for a particular key are routed by intermediate CAN nodes towards the CAN node whose zone contains that key
- a virtual d-dimensional Cartesian coordinate space on a d-torus (a surface of revolution generated by revolving a circle in three-dimensional space about an axis that is coplanar with the circle)
- In a d-dimensional coordinate space, two nodes are neighbors if their coordinate spans overlap along d1 dimensions and abut along one dimension
- To store a pair (K1 ,V1), key K1 is deterministically mapped onto a point P in the coordinate space using a uniform hash function. The corresponding (key,value) pair is then stored at the node that owns the zone within which the point P lies
- To retrieve an entry corresponding to key K1, any node can apply the same deterministic hash function to map K1 onto point P and then retrieve the corresponding value from the point P
- Using its neighbor coordinate set, a node routes a message towards its destination by simple greedy forwarding to the neighbor with coordinates closest to the destination coordinates.
- Node join is done by an existing node splitting its allocated zone in half, retaining half and handing the other half to the new node
- Node departure is done by explicitly handing over the zone and associated KV database to one of its neighbors
- Node failure handled through an immediate takeover algorithm that ensures one of the failed node’s neighbors takes over the zone. However in this case the (key,value) pairs held by the departing node are lost until the state is refreshed by the holders of the data
- Sends heartbeat messages to neighbours. Absence of this over a period of time indicates failure 

        ** Chord **
- a scalable protocol for lookup in a dynamic peer-to-peer system with frequent node arrivals and departures
- Three features that distinguish Chord from many other peer-topeer lookup protocols are its simplicity, provable correctness, and
provable performance
- given a node, maps a node IP to an id
- Given a key, maps the key to a node
- communication cost and state per node scales logarithmically
- node info: (O logN) nodes
- lookup: (O logN) messages
- node changes: (O log2 N) messages

* Key structure
- no naming structure is imposed
- consistent hashing using an m-bit ID (m being possible upper bound on nodes):
	- node ID is created by hashing the IP address
	- key id is created by hashing original key
- assigning keys into node
	- key is assigned to first node whose ID is equal or the next k (successor)
	- successors are identified in a clockwise manner

* environment changes
- node join
	- keys equal to node assigned to node and successor keys for node are given to node
- node departure. (pred, self]
	- all assigned keys are re-assigned to successor

* mitigating adversarial changes
- intentional hashing of keys to the same ID
	- use of SHA-1 function. Making a set of colliding keys is believed hard to accomplish thus it is based on "standard hardness assumptions" that each node is responsible for at most 1 + e)K/N keys



** virtual nodes **
- a solution to balancing machine load allocation in the node ring
- each real machine pretends to be many distinct machines. These operate independently in the DHT ring.

* drawbacks [Karger, Ruhl]
- real node must alocate space for data of each virtual node. More data space. However, this space requirement might be seen as negligable as these p2p structures need logarithmic space per node
- Network bandwidth as every virtual node must frequently do liveness check on neighbours, replacing neighbours if liveness check fails. Many virtual nodes creates an increase in bandwidth load.



        ** DHT based protocols **
* drawbacks
- churn does cause significant overhead for DHTs. [Chawathe, Ratnasamy]
	- to preserve the efficiency and correctness of routing, most DHTs require O(log n) repair operations after each failure.
	- Graceless failures, where a node fails without beforehand informing its neighbors and transferring the relevant state, require more time and work in DHTs to (a) discover the failure and (b) rereplicate the lost data or pointers
	- If the churn rate is too high, the overhead caused by these repair operations can become substantial and could easily overwhelm nodes with low-bandwidth connections


        ** Skip Graph **
Because hashing destroys the ordering on keys, DHT systems do not support queries that seek near matches to a key, or to keys within a given range
Skip graphs can also be constructed without knowledge of the total number of nodes in advance. In contrast, DHT systems such as Pastry and Chord require a priori knowledge about the size of the system or its keyspace



        ** alternative load balancing algorithms for p2p systems [Karger, Ruhl] **
* shortcomings of load balancing
- “random” partition of the address space to nodes is not completely balanced. Some nodes end up responsible for a larger portion of the addresses and thus receive a larger portion of the randomly distributed items.
- some applications may preclude the randomization of data items’ addresses. For example, to support range searching in a database application the items may need
to be placed in a specific order, or even at specific addresses, on the ring. In such cases, we may find the items unevenly distributed in address space, meaning that balancing the address space among nodes does not balance the distribution of items among nodes.
